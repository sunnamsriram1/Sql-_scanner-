#!/data/data/com.termux/files/usr/bin/bash

# ðŸ” Auto SQLMap Crawler Script + URL Extractor Saver
# ðŸ’¡ Saves crawled URLs to a file
# ðŸ” Coded by Sriram

clear
echo -e "\033[1;36mðŸ›¡ï¸ Simple SQLMap Auto-Crawler Script â€” Coded by Sriram"
echo -e "\033[1;33mEnter target URL (example: http://site.com): \033[0m"
read target

if [[ -z "$target" ]]; then
    echo -e "\033[1;31m[âœ˜] No URL entered! Exiting..."
    exit 1
fi

# ðŸ” Check if sqlmap is available
if [[ ! -f sqlmap/sqlmap.py ]]; then
    echo -e "\033[1;31m[âœ˜] sqlmap not found in ./sqlmap directory!"
    echo -e "\033[1;33m[â†’] Run ./install_update.sh to install sqlmap first.\033[0m"
    exit 1
fi

# ðŸ“„ Output files
outfile="crawled_urls.txt"
tmpfile="tmp_sqlmap_output.txt"

echo -e "\n\033[1;32m[âœ“] Crawling: $target with sqlmap --crawl=2 --batch\n"
python3 sqlmap/sqlmap.py -u "$target" --crawl=2 --batch > "$tmpfile"

# ðŸ”Ž Extract URLs with parameters
grep -oE 'http[s]?://[^ ]+\?(id|nid|page|cat|newsid|pid)=[^ ]+' "$tmpfile" | sort -u >> "$outfile"

echo -e "\n\033[1;36m[âœ“] Extracted URLs saved to: $outfile\033[0m"
rm "$tmpfile"
